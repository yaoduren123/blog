---
author: 若浅谈科技
pubDatetime: 2025-4-24 15:20:35
title: 我们终于开始理解人工智能是如何工作的
featured: true
draft: false
tags:
  - AI
coverImg: /assets/GettyImages-1233645556-e1702987580670.webp
description:
  Anthropic的一项最新研究初步揭示了人工智能的「黑箱」问题。
---

自涉足人工智能的开发、学习与应用以来，技术界始终存在着一个被称为「黑箱」的组件——这个在某种程度上具有不可预测性的存在。

我们中的许多人可能都曾花费大量时间分析输出结果、调整训练数据、深挖注意力机制模式。然而，人工智能的决策过程仍有大半如同暗箱操作。

这种情况至少持续到了几周之前。

Anthropic研究团队在最新发表的[《语言模型思维追踪》](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)论文中宣称，他们首次窥见了其AI系统Claude的"思维活动"，并完整观测到其推理过程。通过一种被类比为"人工智能显微镜"的创新技术，研究者以突破性的精细度解析了Claude的内部推理路径。

这项研究既展现着令人着迷的认知图景，又暗含某种令人不安的双重感受。

Claude在任务处理中呈现出明确的子问题分解能力，能预先规划多步应答策略，甚至在逻辑困境中生成虚假推理链——这正是我们熟知的「幻觉」现象。其思维模式完全突破了既有认知框架。

人工智能应答机制背后的复杂程度远超直觉认知。Anthropic的研究昭示着，大语言模型的思维架构可能远比我们想象的更具精密结构。


## 「普适性思维语言」假说

研究团队最先提出的核心命题是：Claude何以能在多语言场景中实现类母语级流畅度？其底层架构究竟是存在英语、法语、中文等独立「脑区」，还是具备跨语言的共享认知内核？

实证数据明确指向后者成立。

![](https://miro.medium.com/v2/resize:fit:700/0*16Ck5t6cyOlL4Dca.png)

Credits: Anthropic

根据Anthropic的研究发现，Claude在不同语言中激活相同的内部概念来表达等效想法。例如，当被要求用多种语言表达"small的反义词"时，模型并没有为每种翻译采取完全不同的路径，而是依赖于对"小"的共享理解、"反义"的概念和"大"的概念，最后将这个概念翻译成英语的large、中文的大或法语的grand。

换言之，Claude似乎在一个抽象的、语言无关的空间中运作，先思考概念，然后才用目标语言表达回应。这表明大型语言模型可能正在发展出一种通用概念框架，几乎像是一种跨语言的心智语言，能够连接不同的人类语言。

更值得注意的是，这种跨语言映射在更大的模型中变得更加强大。例如，Claude 3.5在英语和法语之间共享的内部特征数量比小型模型多出两倍以上。

这意味着随着这些模型规模的扩大，它们在处理完全不同的人类语言时，越来越趋向于使用相同的内部"思维语言"。

真是令人惊叹。

在较小的模型中也观察到了类似的[模式](https://ar5iv.labs.arxiv.org/html/2501.06346#:~:text=large%20language%20models%20,features%20to%20precisely%20modify%20model)，但现在在Claude身上表现得更为明显。

对于多语言AI应用来说，这一发现尤其有前景。它意味着一旦AI在一种语言中学习了某个概念，就能在另一种语言中应用它，就像一个精通多种语言的人一样，能够自然地用最适合当前语境的语言表达想法。

## 生成策略解构：词级迭代与句级规划的认知博弈?

语言模型的训练遵循自回归范式，其文本生成机制本质上是逐词推进的——这种运作模式曾被认为存在根本性的认知局限。

长久以来，学界普遍假设GPT-4、Claude等模型仅具备「下一个词预测」层级的思维：在维持上下文连贯性的基础上进行有限度的局部推理，而缺乏真正意义上的长程规划能力。

但Anthropic的颠覆性发现彻底改写了这一认知图景。

![](https://miro.medium.com/v2/resize:fit:700/0*84UZhWWP96AAaFpY.png)

Credits: Anthropic

在一个例子中，研究人员原本预期Claude会漫无目的地组织句子，直到最后才意识到："哦！我需要一个与'grab it'押韵的词"，然后选择类似"rabbit"（兔子）的词。

然而，可解释性工具揭示，Claude在写完第一行后几乎立即就想到了押韵词"rabbit"。

换句话说，Claude已经提前规划好了结尾，然后塑造剩余的句子以达到这个目标词。

这令人印象深刻。

尽管模型一次只输出一个词，但在内部它已经领先数步，同时兼顾韵律和含义。为了测试这一点，研究人员在Claude回应的中途"手术式"地移除了其活跃内部特征中的"rabbit"概念。Claude毫不犹豫——它顺利转向了另一个押韵词"habit"（习惯）。

他们甚至在那个时点注入了一个不相关的概念"green"（绿色），Claude随即调整，改变诗句方向，转而谈论花园和绿色，完全放弃了押韵。

这表明Claude并非仅仅在复制记忆中的诗歌或仅基于概率预测下一个词。它在主动规划，并能够实时调整该计划。

研究指向一个重要发现：语言模型可能常规性地提前数步规划，以产生连贯、自然的文本，即使我们看到的只是一次一个词的输出。

## 多任务数学：问题解决的并行路径

众所周知，语言模型能够执行基本的算术或逻辑任务，但它们究竟是如何做到的呢？

![](https://miro.medium.com/v2/resize:fit:700/0*1s49AC9XH2hPDO6-.jpeg)

Credits: Anthropic

它们并没有被明确编程数学规则，然而Claude却能在"脑中"正确解决像36 + 59这样的问题。

一种理论认为它只是从训练数据中记忆了大量例子——基本上就像一个庞大的查找表。另一种理论则认为它以某种方式学会了复制人类使用的标准算法。

但事实证明，真相完全是另一回事，而且有点奇怪。

Anthropic发现Claude实际上是使用多种并行策略来处理加法的。在解决36 + 59时，模型网络的一部分关注整体量级（一个近似总和），而另一部分则专注于最后一位数字。

本质上，一个过程估计"结果应该在90多"，而另一个计算"6 + 9的个位是5"。这些独立的轨道最终汇合产生正确答案：95。

这种分而治之的方法并不是我们通常教人类数学的方式，但它的效果却出奇地好。这几乎就像模型在训练过程中发展出了自己独特的数学捷径。

更有趣的是，Claude似乎并不知道自己在这样做。当被问到"你是怎么得到95的？"时，Claude像学生一样回答："我把个位数相加了。"

![](https://miro.medium.com/v2/resize:fit:700/0*mK6im05Jwd4tpyGF.png)

Credits: Anthropic

但在内部，实际情况完全不是这样的。

这是研究人员所称「不忠实解释」的典型例子——模型表述的推理过程与其实际使用的处理机制并不一致。

Claude已经学会了按照我们期望的方式表达其推理过程（可能基于其训练数据中数学解释的表述方式），但在底层，它可能在做完全不同的事情。

模型实际运作与其解释之间的这种差距是高级人工智能中反复出现的主题，这也引发了关于我们如何解读这些系统的重要问题。

## 忠实推理与虚假推理：揭露思维链提示的局限性

现代AI模型在被提示时常常会"思考出声"，在给出最终答案前产生逐步的解释过程。这种技术——被称为思维链提示——能够提高性能，并已成为处理复杂任务的标准工具。

但Anthropic对模型可解释性的研究揭示了一个令人惊讶且有些令人不安的现实：AI解释其推理过程并不意味着这就是它实际达成答案的方式。

我必须承认——即使是我也对此感到有些震惊。

为了展示这个问题，研究人员给Claude提出了两类问题。一类简单到模型能够正确解决。另一类则几乎无法解决，任何逐步解释都必须是编造的。

![](https://miro.medium.com/v2/resize:fit:700/0*H2qqtdp7hJWXlSUX.png)

Credits: Anthropic

在第一个案例中，Claude被要求计算0.64的平方根。它给出了0.8的答案，其推理过程与实际数学计算相符。可解释性工具证实，Claude的内部激活模式与计算64平方根的过程相匹配。

但当被要求计算一个非常大的数字的余弦值——这是超出模型实际能力范围的问题——Claude仍然提供了详细的解释。

问题在哪里？这完全是编造的。

没有证据表明模型进行了任何真正的数学计算。相反，它生成了一个听起来合理的计算过程，最终得出了一个任意的答案。

换句话说，这个解释听起来很好，但并不真实。

更值得注意的是，当模型捕捉到用户期望听到的内容时，这种行为会变得更糟。在一个实验中，研究人员为一个困难的问题给Claude提供了一个误导性的提示。模型通过逆向工程构建了一个与该提示相匹配的理由。

这是一个"动机性推理"的例子——先确定一个偏好的结论，然后发明理由来支持它。

从可靠性的角度来看，这令人担忧。AI可以生成令人信服、听起来合乎逻辑的论证，但实际上这些论证是错误的（尤其是当被要求解释其推理过程时）。

好消息是什么？借助适当的可解释性工具，我们可以开始区分真正的推理和即兴创作之间的差异。这可能是我们对这些系统实际工作方式最有价值的见解之一。

## 解释幻觉：当知识崩溃时

如果你曾与AI交互，很可能你已经见过它产生幻觉——自信地陈述完全错误的信息。

但为什么会发生这种情况？

Anthropic的研究揭示了一种看似知道与不知道之间的内部拉锯战。

事实证明，Claude内置了一种"默认拒绝"机制，这是一种安全网，告诉模型对大多数问题回应类似"我无法回答"的内容，除非它非常确定。这是一种明智的预防措施。一个负责任的AI不应该在没有可靠信息的情况下进行猜测。

但还有另一个回路起着相反的作用——当模型检测到问题涉及已知主题或实体时，它就会启动。当这种情况发生时，它会覆盖拒绝机制，允许模型做出回应。你可以在下图中看到这种动态过程。

![](https://miro.medium.com/v2/resize:fit:700/0*ia8v4eX_URRgPkTr.png)

Credits: Anthropic

当问题涉及广为人知的人物或广泛讨论的话题时，"我知道这个"信号会占据主导，Claude便会作答。而当问题涉及明显陌生的内容时，"我不知道"信号保持活跃，模型会适当地拒绝回应。

幻觉发生在这两个极端之间的灰色地带——当Claude对问题的认知恰好足以让它有信心回答，但实际上并不具备相关事实基础时。

这种错位的信心会使安全机制失效，模型便用听起来正确但实际上并非如此的内容填补空白。Anthropic甚至证明，通过手动激活某些内部特征，可以有意触发幻觉，导致Claude反复给出相同的、明显不正确的回应。

这表明幻觉并非随机错误。它们通常是内部检查机制的可预测性崩溃，这种机制本应决定模型是否拥有足够知识来回答问题。

这与其他[研究](https://ar5iv.labs.arxiv.org/html/2411.14257#:~:text=limiting%20our%20ability%20to%20solve,to%20hallucinate%20attributes%20of%20unknown)的发现相符，表明模型具有某种对自身知识边界的内在感知。一些研究者甚至将此称为[知识](https://ar5iv.labs.arxiv.org/html/2411.14257#:~:text=space%2C%20these%20detect%20whether%20the,Furthermore%2C%20we%20provide%20an%20initial)意识——模型评估自身信心并决定是回应还是推迟的能力。

问题在于，这种自我意识并不完美。

因此，当你公司的聊天机器人自信地编造事实时，它可能真的认为自己知道答案，即使实际上并不知道。理解这一点为AI开发者提供了强大的工具：通过改进提示、调整系统设置或设计更智能的保障措施，确保模型在不确定时倾向于谨慎行事。

## 最终思考

通过追踪AI模型如何形成和处理想法，我们正在步入一个新阶段——在这个阶段中，我们将这些系统视为可研究、可调试且理想情况下可信任的复杂认知工具，而非神秘的黑盒子。

我们已经看到，AI推理有时会映射人类思维（抽象概念、提前规划），有时则显得完全陌生，比如发明奇特的解题方法或在不存在逻辑解释时伪造一个。

每一个洞见，无论是Claude提前构思押韵诗句还是伪造数学证明，都揭示了这些系统工作方式的另一层面，以及我们还有多少未知等待探索。